# rag_backend.py
import os
import warnings
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# --- æ­£ç¡®çš„å¯¼å…¥æ–¹å¼ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from embeddings import GiteeAIEmbeddings  # ä½¿ç”¨ Gitee AI Embeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

print("âœ… åç«¯æ¨¡å—å·²åŠ è½½ (å·²å¯ç”¨å…¨å±€å†…å­˜ç¼“å­˜)")

# ==========================================
# ğŸš€ å…¨å±€ç¼“å­˜å­—å…¸
# ==========================================
# è¿™æ˜¯ä¸€ä¸ªå­˜æ”¾åœ¨å†…å­˜é‡Œçš„å­—å…¸ï¼Œç”¨æ¥ä¿å­˜å¤„ç†å¥½çš„å‘é‡åº“
# Key (é”®): æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ "temp/doc.pdf")
# Value (å€¼): å¤„ç†å¥½çš„ FAISS å‘é‡åº“å¯¹è±¡
VECTOR_STORE_CACHE = {}


def get_vectorstore(file_path, api_key):
    """
    æ ¸å¿ƒåŠ©æ‰‹å‡½æ•°ï¼šè·å–å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰
    
    é€»è¾‘ï¼š
    1. å…ˆçœ‹ç¼“å­˜é‡Œæœ‰æ²¡æœ‰ã€‚
    2. æœ‰çš„è¯ï¼Œç›´æ¥æ‹¿æ¥ç”¨ï¼ˆç§’å›ï¼ï¼‰ã€‚
    3. æ²¡æœ‰çš„è¯ï¼Œæ‰å»è¾›è‹¦åŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åŒ–ï¼Œç„¶åå­˜è¿›ç¼“å­˜ä¾›ä¸‹æ¬¡ç”¨ã€‚
    """
    global VECTOR_STORE_CACHE
    
    # --- 1. æ£€æŸ¥ç¼“å­˜ ---
    if file_path in VECTOR_STORE_CACHE:
        print(f"âš¡ [ç¼“å­˜å‘½ä¸­] å‘ç°å·²å¤„ç†è¿‡çš„æ–‡æ¡£: {file_path}")
        print("   -> è·³è¿‡åŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åŒ–ï¼Œç›´æ¥å¤ç”¨ï¼")
        return VECTOR_STORE_CACHE[file_path]
    
    # --- 2. ç¼“å­˜æœªé€‰ä¸­ï¼Œå¼€å§‹å¤„ç† ---
    print(f"ğŸ“¥ [ç¼“å­˜æœªå‘½ä¸­] è¿™æ˜¯ä¸€ä¸ªæ–°æ–‡æ¡£ï¼Œå¼€å§‹å®Œæ•´å¤„ç†æµç¨‹: {file_path}")
    try:
        # A. åŠ è½½ä¸åˆ‡åˆ†
        print("   1. åŠ è½½ PDF æ–‡ä»¶...")
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        
        print("   2. åˆ‡åˆ†æ–‡æœ¬...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        print(f"      å…±åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")
        
        # B. å‘é‡åŒ–
        print("   3. åˆå§‹åŒ– Gitee AI åµŒå…¥æ¨¡å‹...")
        embeddings = GiteeAIEmbeddings(
            api_key=api_key,
            model="Qwen3-Embedding-8B",
            base_url="https://ai.gitee.com/v1"
        )
        
        print("   4. åˆ›å»º FAISS å‘é‡å­˜å‚¨ (è¿™æ­¥æœ€è€—æ—¶)...")
        vectorstore = FAISS.from_documents(
            documents=splits, 
            embedding=embeddings
        )
        print("      âœ… å‘é‡åº“æ„å»ºå®Œæˆ")
        
        # --- 3. å­˜å…¥ç¼“å­˜ ---
        VECTOR_STORE_CACHE[file_path] = vectorstore
        print(f"ğŸ’¾ [å·²ç¼“å­˜] æ–‡æ¡£å·²å­˜å…¥å…¨å±€å†…å­˜ï¼Œä¸‹æ¬¡æé—®å°†ç§’å›ï¼")
        
        return vectorstore

    except Exception as e:
        print(f"âŒ å‘é‡åº“åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def get_llm():
    """è¾…åŠ©å‡½æ•°ï¼šè·å– LLM å®ä¾‹"""
    return ChatOpenAI(
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        model="ep-20251122233041-rpp9j",
        temperature=0.1
    )


def ask_document(file_path, query, api_key):
    """
    åŸºäº PDF æ–‡æ¡£å›ç­”é—®é¢˜ (æ™®é€šç‰ˆ)
    """
    try:
        # 1. è·å–å‘é‡åº“ (æ™ºèƒ½ç¼“å­˜ç‰ˆ)
        vectorstore = get_vectorstore(file_path, api_key)
        
        # 2. æ£€ç´¢
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³ç‰‡æ®µ...")
        relevant_docs = vectorstore.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # 3. ç”Ÿæˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        llm = get_llm()
        
        prompt = ChatPromptTemplate.from_template("""                
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        
        è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¶…å‡ºæ–‡æ¡£çš„ä¿¡æ¯ï¼š
        
        {context}
        
        å¯¹è¯å†å²å’Œç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š
        {query}
        
        å›ç­”ï¼š
        """)
        
        messages = prompt.format_messages(context=context, query=query)
        response = llm.invoke(messages)
        
        return response.content
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        raise


def ask_document_stream(file_path, query, api_key):
    """
    åŸºäº PDF æ–‡æ¡£å›ç­”é—®é¢˜ (æµå¼ç‰ˆ)
    """
    try:
        # 1. è·å–å‘é‡åº“ (æ™ºèƒ½ç¼“å­˜ç‰ˆ)
        vectorstore = get_vectorstore(file_path, api_key)
        
        # 2. æ£€ç´¢
        relevant_docs = vectorstore.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # 3. ç”Ÿæˆå›ç­” (æµå¼)
        # ä¸“é—¨åˆ›å»ºä¸€ä¸ªæµå¼çš„ LLM å¯¹è±¡
        llm_stream = ChatOpenAI(
            base_url=os.getenv("DEEPSEEK_BASE_URL"),
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            model="ep-20251122233041-rpp9j",
            temperature=0.1,
            streaming=True
        )
        
        prompt = ChatPromptTemplate.from_template("""                
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        
        è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¶…å‡ºæ–‡æ¡£çš„ä¿¡æ¯ï¼š
        
        {context}
        
        å¯¹è¯å†å²å’Œç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š
        {query}
        
        å›ç­”ï¼š
        """)
        
        messages = prompt.format_messages(context=context, query=query)
        
        for chunk in llm_stream.stream(messages):
            if chunk.content:
                yield chunk.content
        
    except Exception as e:
        print(f"æµå¼ç”Ÿæˆé”™è¯¯: {str(e)}")
        yield f"âŒ å‡ºé”™å•¦ï¼š{str(e)}"

if __name__ == "__main__":
    print("è¿™æ˜¯åç«¯æ¨¡å—ï¼Œè¯·è¿è¡Œ app.py")
